import streamlit as st
import pandas as pd
import os
import re

# --- ‡∏Å‡∏≤‡∏£‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤‡∏´‡∏ô‡πâ‡∏≤‡∏Å‡∏£‡∏∞‡∏î‡∏≤‡∏© ---
st.set_page_config(page_title="Sensor Quality Analysis", layout="wide")

# --- ‡∏™‡πà‡∏ß‡∏ô‡∏Ç‡∏≠‡∏á Sidebar ---
with st.sidebar:
    st.title("üìÇ ‡∏Å‡∏≤‡∏£‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£‡πÑ‡∏ü‡∏•‡πå")
    uploaded_files = st.file_uploader("‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡πÑ‡∏ü‡∏•‡πå CSV", type=['csv'], accept_multiple_files=True)
    st.divider()
    st.write("‚öôÔ∏è **Settings**")
    # ‡∏õ‡∏£‡∏±‡∏ö‡πÄ‡∏Å‡∏ì‡∏ë‡πå‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ï‡πà‡∏≤‡∏á‡∏à‡∏≤‡∏Å‡∏Ñ‡πà‡∏≤‡∏õ‡∏Å‡∏ï‡∏¥
    spike_limit = st.number_input("‡πÄ‡∏Å‡∏ì‡∏ë‡πå‡∏Å‡∏≤‡∏£‡∏û‡∏∏‡πà‡∏á‡∏ú‡∏¥‡∏î‡∏õ‡∏Å‡∏ï‡∏¥ (Value Spike)", value=100.0)
    st.info(f"‡∏£‡∏∞‡∏ö‡∏ö‡∏à‡∏∞‡∏ô‡∏±‡∏ö‡πÄ‡∏õ‡πá‡∏ô Error ‡πÄ‡∏°‡∏∑‡πà‡∏≠:\n1. ‡∏Ñ‡πà‡∏≤‡∏û‡∏∏‡πà‡∏á‡πÄ‡∏Å‡∏¥‡∏ô‡∏Å‡∏ß‡πà‡∏≤‡∏Ñ‡πà‡∏≤‡∏õ‡∏Å‡∏ï‡∏¥‡∏£‡∏≠‡∏ö‡∏Ç‡πâ‡∏≤‡∏á (Median) ‡πÄ‡∏Å‡∏¥‡∏ô {spike_limit}\n2. ‡∏Ñ‡πà‡∏≤‡∏™‡∏π‡∏á‡πÄ‡∏Å‡∏¥‡∏ô 6600")

# --- ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• ---
def get_analysis_data(df, suffix, dht_cols, piera_cols, limit):
    # ‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏°‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏ö‡∏∑‡πâ‡∏≠‡∏á‡∏ï‡πâ‡∏ô
    df['datetime'] = pd.to_datetime(df['datetime'], format='%d-%m-%Y-%H-%M-%S', errors='coerce')
    df = df.dropna(subset=['datetime']).sort_values('datetime') 
    
    # --- 1. ‡∏´‡∏≤ Time Gap (‡∏ä‡πà‡∏ß‡∏á‡∏ó‡∏µ‡πà‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏´‡∏≤‡∏¢‡∏ô‡∏≤‡∏ô‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î) ---
    df['time_diff'] = df['datetime'].diff().dt.total_seconds()
    max_gap = df['time_diff'].max() - 1 if df['time_diff'].max() > 1 else 0
    gap_info = "‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏Ç‡∏≤‡∏î‡∏´‡∏≤‡∏¢"
    if max_gap > 0:
        gap_idx = df['time_diff'].idxmax()
        gap_end_time = df.loc[gap_idx, 'datetime']
        prev_idx_pos = df.index.get_loc(gap_idx) - 1
        gap_start_time = df.iloc[prev_idx_pos]['datetime']
        gap_info = f"{int(max_gap)} ‡∏ß‡∏¥‡∏ô‡∏≤‡∏ó‡∏µ ({gap_start_time.strftime('%H:%M:%S')} - {gap_end_time.strftime('%H:%M:%S')})"

    # --- 2. ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö Outlier (‡∏Ñ‡πà‡∏≤‡∏û‡∏∏‡πà‡∏á‡∏ú‡∏¥‡∏î‡∏õ‡∏Å‡∏ï‡∏¥) ---
    pm_col = next((c for c in df.columns if 'PM2' in c and '5' in c), None)
    error_info = "‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏Ñ‡πà‡∏≤‡∏ú‡∏¥‡∏î‡∏õ‡∏Å‡∏ï‡∏¥"
    total_errors = 0
    
    if pm_col:
        # --- Logic ‡πÉ‡∏´‡∏°‡πà: ‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö‡∏Ñ‡πà‡∏≤‡∏õ‡∏±‡∏à‡∏à‡∏∏‡∏ö‡∏±‡∏ô‡∏Å‡∏±‡∏ö Rolling Median ---
        pm_valid = df[['datetime', pm_col]].dropna().copy()
        
        # ‡∏´‡∏≤‡∏Ñ‡πà‡∏≤‡∏õ‡∏Å‡∏ï‡∏¥‡∏£‡∏≠‡∏ö‡∏Ç‡πâ‡∏≤‡∏á (‡πÉ‡∏ä‡πâ 11 ‡πÅ‡∏ñ‡∏ß‡∏£‡∏≠‡∏ö‡πÜ)
        pm_valid['baseline'] = pm_valid[pm_col].rolling(window=11, center=True, min_periods=1).median()
        
        # ‡πÄ‡∏ä‡πá‡∏Ñ‡∏ß‡πà‡∏≤‡∏Ñ‡πà‡∏≤‡∏õ‡∏±‡∏à‡∏à‡∏∏‡∏ö‡∏±‡∏ô '‡∏û‡∏∏‡πà‡∏á‡∏™‡∏π‡∏á' ‡∏Å‡∏ß‡πà‡∏≤‡∏Ñ‡πà‡∏≤‡∏õ‡∏Å‡∏ï‡∏¥‡∏£‡∏≠‡∏ö‡∏Ç‡πâ‡∏≤‡∏á‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà
        # (‡πÉ‡∏ä‡πâ‡∏Ñ‡πà‡∏≤‡∏•‡∏ö‡∏Å‡∏±‡∏ô‡∏ï‡∏£‡∏á‡πÜ ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏î‡∏π‡∏ß‡πà‡∏≤‡∏°‡∏±‡∏ô‡πÇ‡∏î‡∏î‡∏Ç‡∏∂‡πâ‡∏ô‡πÑ‡∏õ ‡πÑ‡∏°‡πà‡πÉ‡∏ä‡πà‡∏ï‡∏≠‡∏ô‡∏°‡∏±‡∏ô‡∏ï‡∏Å‡∏•‡∏á‡∏°‡∏≤)
        pm_valid['is_error'] = (pm_valid[pm_col] > 6600) | ((pm_valid[pm_col] - pm_valid['baseline']) > limit)
        
        # ‡∏ô‡∏±‡∏ö‡∏à‡∏≥‡∏ô‡∏ß‡∏ô Error
        total_errors = pm_valid['is_error'].sum()
        if total_errors > 0:
            err_times = pm_valid[pm_valid['is_error'] == True]['datetime'].dt.strftime('%H:%M:%S').unique()
            error_info = f"‡∏û‡∏ö {total_errors} ‡∏Ñ‡∏£‡∏±‡πâ‡∏á (‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á: {', '.join(err_times[:3])})"
        
        # ‡∏ô‡∏≥‡∏™‡∏ñ‡∏≤‡∏ô‡∏∞ Error ‡∏Å‡∏•‡∏±‡∏ö‡πÑ‡∏õ‡∏£‡∏ß‡∏°‡∏Å‡∏±‡∏ö‡∏ï‡∏≤‡∏£‡∏≤‡∏á‡∏´‡∏•‡∏±‡∏Å
        df = df.merge(pm_valid[['datetime', 'is_error']], on='datetime', how='left')
        df[f'pm_error_{suffix}'] = df['is_error'].fillna(False).astype(int)
    else:
        df[f'pm_error_{suffix}'] = 0
        total_errors = 0

    # --- 3. ‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏°‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏™‡∏ñ‡∏≤‡∏ô‡∏∞‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÅ‡∏™‡∏î‡∏á‡∏ú‡∏•‡∏£‡∏≤‡∏¢‡∏ô‡∏≤‡∏ó‡∏µ ---
    status_df = pd.DataFrame()
    status_df['datetime'] = df['datetime']
    status_df[f'has_dt_{suffix}'] = 1
    status_df[f'has_dht_{suffix}'] = df[dht_cols].notnull().any(axis=1).astype(int)
    status_df[f'has_piera_{suffix}'] = df[piera_cols].notnull().any(axis=1).astype(int)
    status_df[f'both_up_{suffix}'] = ((status_df[f'has_dht_{suffix}'] == 1) & (status_df[f'has_piera_{suffix}'] == 1)).astype(int)
    status_df[f'pm_error_{suffix}'] = df[f'pm_error_{suffix}']
    
    return status_df.groupby('datetime').max(), gap_info, total_errors, error_info

# --- ‡∏™‡πà‡∏ß‡∏ô‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤‡∏´‡∏•‡∏±‡∏Å ---
st.title("üìä Sensor Data Quality & Outlier Analysis")

if uploaded_files:
    data_groups = {} 
    for file in uploaded_files:
        name = file.name
        date_match = re.search(r'(\d{2}-\d{2}-\d{4})', name)
        if date_match:
            date_key = date_match.group(1)
            point_id = "P1" if "Point01" in name else "P2"
            if date_key not in data_groups: data_groups[date_key] = {}
            data_groups[date_key][point_id] = file

    for date_key in sorted(data_groups.keys()):
        st.subheader(f"üìÖ ‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà: {date_key}")
        group = data_groups[date_key]
        
        if 'P1' in group and 'P2' in group:
            df1, df2 = pd.read_csv(group['P1']), pd.read_csv(group['P2'])
            dht_cols, piera_cols = ['humidity', 'temperature'], [c for c in df1.columns if c.startswith('PC') or c.startswith('PM')]

            status_p1, gap_p1, err_count_p1, err_time_p1 = get_analysis_data(df1, 'P1', dht_cols, piera_cols, spike_limit)
            status_p2, gap_p2, err_count_p2, err_time_p2 = get_analysis_data(df2, 'P2', dht_cols, piera_cols, spike_limit)
            
            combined = pd.concat([status_p1, status_p2], axis=1)
            analysis_1min = combined.fillna(0).resample('1min').sum()
            analysis_1min['missing_P1'] = (60 - analysis_1min['has_dt_P1']).clip(lower=0)
            analysis_1min['missing_P2'] = (60 - analysis_1min['has_dt_P2']).clip(lower=0)

            st.info(f"üìà **Raw Data:** P1: {len(df1):,} ‡πÅ‡∏ñ‡∏ß | P2: {len(df2):,} ‡πÅ‡∏ñ‡∏ß")
            
            # --- ‡πÅ‡∏ñ‡∏ß‡∏ó‡∏µ‡πà 1 ---
            col1, col2, col3, col4 = st.columns(4)
            with col1: st.metric("‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏Ç‡πâ‡∏≤‡πÄ‡∏â‡∏•‡∏µ‡πà‡∏¢/‡∏ô‡∏≤‡∏ó‡∏µ", f"{analysis_1min['has_dt_P1'].mean():.2f} | {analysis_1min['has_dt_P2'].mean():.2f}")
            with col2: st.metric("Avg DHT", f"{analysis_1min['has_dht_P1'].mean():.2f} | {analysis_1min['has_dht_P2'].mean():.2f}")
            with col3: st.metric("Avg Piera", f"{analysis_1min['has_piera_P1'].mean():.2f} | {analysis_1min['has_piera_P2'].mean():.2f}")
            with col4: st.metric("‡∏Ñ‡∏£‡∏ö 2 ‡πÄ‡∏ã‡∏ô‡πÄ‡∏ã‡∏≠‡∏£‡πå", f"{analysis_1min['both_up_P1'].mean():.2f} | {analysis_1min['both_up_P2'].mean():.2f}")

            # --- ‡πÅ‡∏ñ‡∏ß‡∏ó‡∏µ‡πà 2 ---
            st.markdown("#### ‚ö†Ô∏è ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏Ñ‡∏∏‡∏ì‡∏†‡∏≤‡∏û‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• (Quality Check)")
            c_miss, c_gap, c_err = st.columns([1, 1.5, 1])
            with c_miss:
                st.metric("‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏™‡∏π‡∏ç‡∏´‡∏≤‡∏¢‡∏£‡∏ß‡∏° (P1|P2)", f"{int(analysis_1min['missing_P1'].sum()):,} | {int(analysis_1min['missing_P2'].sum()):,}", delta="‡∏ß‡∏¥‡∏ô‡∏≤‡∏ó‡∏µ", delta_color="inverse")
            with c_gap:
                st.write(f"‚åõ **‡∏ä‡πà‡∏ß‡∏á‡∏ó‡∏µ‡πà‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏´‡∏≤‡∏¢‡∏ô‡∏≤‡∏ô‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î:**")
                st.write(f"‚Ä¢ P1: {gap_p1}")
                st.write(f"‚Ä¢ P2: {gap_p2}")
            with c_err:
                st.metric("Total Errors (Outliers)", f"{int(err_count_p1):,} | {int(err_count_p2):,}", delta="‡∏à‡∏∏‡∏î‡∏ó‡∏µ‡πà‡∏û‡∏∏‡πà‡∏á", delta_color="inverse")
                st.write(f"üïí **‡πÄ‡∏ß‡∏•‡∏≤‡∏ó‡∏µ‡πà‡∏û‡∏ö‡∏Ñ‡πà‡∏≤‡∏ú‡∏¥‡∏î‡∏õ‡∏Å‡∏ï‡∏¥:**")
                st.caption(f"P1: {err_time_p1}")
                st.caption(f"P2: {err_time_p2}")

            with st.expander("üîç ‡∏î‡∏π‡∏£‡∏≤‡∏¢‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î‡∏£‡∏≤‡∏¢‡∏ô‡∏≤‡∏ó‡∏µ‡πÅ‡∏•‡∏∞‡∏î‡∏≤‡∏ß‡∏ô‡πå‡πÇ‡∏´‡∏•‡∏î"):
                st.dataframe(analysis_1min[['has_dt_P1', 'missing_P1', 'pm_error_P1', 'has_dt_P2', 'missing_P2', 'pm_error_P2']], use_container_width=True)
                csv = analysis_1min.to_csv().encode('utf-8')
                st.download_button("üì• ‡∏î‡∏≤‡∏ß‡∏ô‡πå‡πÇ‡∏´‡∏•‡∏î‡∏™‡∏£‡∏∏‡∏õ (.csv)", data=csv, file_name=f"Summary_{date_key}.csv", key=f"dl_{date_key}")
            st.divider()
        else:
            st.warning(f"‚ö†Ô∏è ‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà {date_key} ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÑ‡∏°‡πà‡∏Ñ‡∏£‡∏ö")
else:
    st.info("üí° ‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡∏≠‡∏±‡∏õ‡πÇ‡∏´‡∏•‡∏î‡πÑ‡∏ü‡∏•‡πå‡∏ó‡∏µ‡πà Sidebar")